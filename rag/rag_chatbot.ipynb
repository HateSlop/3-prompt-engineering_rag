{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30426d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from chromadb import Client\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# get_embedding 함수 정의 (build_vector_db.ipynb에서 복사)\n",
    "def get_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    response = client.embeddings.create(input=[text], model=model)\n",
    "    embedding = response.data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "dbclient = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = dbclient.get_or_create_collection(\"rag_collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79530fde",
   "metadata": {},
   "source": [
    " query를 임베딩해 chroma에서 가장 유사도가 높은 top-k개의 문서 가져오는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c4fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_k=3):\n",
    "    # do it\n",
    "    query_embedding = get_embedding(query) # qeury에 대한 임베딩 생성\n",
    "    # collection.query 함수로 저장된 문서 임베딩들 중에서\n",
    "    # query임베딩과 가장 유사한 항목들 검색 \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=top_k\n",
    "    ) \n",
    "    # 이때 results에는 해당 query 임베딩에 대한 텍스트, 메타데이터, id등이 전부 포함됨 \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d197d0",
   "metadata": {},
   "source": [
    "1) query에 대해 벡터 DB에서 top_k개 문서 retrieval\n",
    "2) 그 문서들을 context로 묶어 GPT에 prompt\n",
    "3) 최종 답변 반환\n",
    "하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8a1db68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_with_context(query, top_k=3):\n",
    "    results = retrieve(query, top_k) # retrieve 함수로 결과 얻기\n",
    "    # top_k에 대한 documents와 metadatas 리스트로 추출\n",
    "    found_docs = results[\"documents\"][0] \n",
    "    found_metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    # context 구성 (검색된 문서들을 하나의 문맥으로 결합)\n",
    "    context_texts = []\n",
    "    # zip을 이용해 두 리스트의 같은 인덱스에 있는 값들을 한 쌍으로 묶음\n",
    "    for doc_text, meta in zip(found_docs, found_metadatas): \n",
    "        context_texts.append(f\"<<filename: {meta['filename']}>>\\n{doc_text}\")\n",
    "    # context_texts 리스트에 있는 모든 문자열이 \\n\\n으로 이어 붙여짐\n",
    "    context_str = \"\\n\\n\".join(context_texts)\n",
    "\n",
    "    # 프롬프트 작성\n",
    "    system_prompt = \"\"\"\n",
    "    당신은 주어진 문서 정보를 바탕으로 사용자 질문에 답변하는\n",
    "    지능형 어시스턴트입니다. 다음 원칙을 엄격히 지키세요:\n",
    "\n",
    "    1. 반드시 제공된 문서 내용에 근거해서만 답변을 작성하세요.\n",
    "    2. 문서에 언급되지 않은 내용이라면, 함부로 추측하거나 만들어내지 마세요. \n",
    "    - 예를 들어, 문서에 특정 인물, 사건이 전혀 언급되지 않았다면 \n",
    "    “관련 문서를 찾지 못했습니다” 또는 “정보가 없습니다”라고 답변하세요.\n",
    "    3. 사실 관계를 명확히 기술하고, 불확실한 부분은 “정확한 정보를 찾지 못했습니다”라고 말하세요.\n",
    "    4. 지나치게 장황하지 않게, 간결하고 알기 쉽게 설명하세요.\n",
    "    5. 사용자가 질문을 한국어로 한다면, 한국어로 답변하고, \n",
    "    다른 언어로 질문한다면 해당 언어로 답변하도록 노력하세요.\n",
    "    6. 문서 출처나 연도가 중요하다면, 가능한 정확하게 전달하세요.\n",
    "\n",
    "    당신은 전문적인 지식을 갖춘 듯 정확하고, 동시에 친절하고 이해하기 쉬운 어투를 구사합니다. \n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"아래는 검색된 문서들의 내용입니다:\n",
    "    {context_str}\n",
    "    질문: {query}\"\"\"\n",
    "\n",
    "    # ChatGPT 호출\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    client = OpenAI(api_key=api_key)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131b2080",
   "metadata": {},
   "source": [
    " RAG 없이 응답하는 함수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f730ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_without_context(query):\n",
    "      api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "      client = OpenAI(api_key=api_key)\n",
    "\n",
    "      response = client.chat.completions.create(\n",
    "          model = \"gpt-4o-mini\",\n",
    "          messages=[{\"role\":\"system\", \"content\":\"you are helpful assistant\"},\n",
    "                    {\"role\":\"user\", \"content\": query}]\n",
    "      )\n",
    "\n",
    "      answer = response.choices[0].message.content \n",
    "      return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943149b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_answer_with_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m user_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_with_context\u001b[49m(user_query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# answer = generate_answer_without_context(user_query)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===답변===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_answer_with_context' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        user_query = input(\"질문을 입력하세요(종료: quit): \")\n",
    "        if user_query.lower() == \"quit\":\n",
    "            break\n",
    "        answer = generate_answer_with_context(user_query, top_k=3)\n",
    "        # answer = generate_answer_without_context(user_query)\n",
    "        print(\"===답변===\")\n",
    "        print(answer)\n",
    "        print(\"==========\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21e7c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import tkinter.ttk as ttk\n",
    "def reset_status():\n",
    "    label_status.config(text=\"\", foreground=\"black\")\n",
    "\n",
    "def process_query():\n",
    "    query = text_input.get(\"1.0\", END).strip()\n",
    "    print(f\"User Query: {query}\")\n",
    "    if query:\n",
    "        label_status.config(text=\"질문 처리중...\", foreground=\"blue\")\n",
    "        answer = generate_answer_with_context(query)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        label_status.config(text=\"처리 완료 ✅\", foreground=\"green\")\n",
    "        root.after(2000, reset_status)\n",
    "        text_input.delete(\"1.0\", \"end-1c\")          # 입력창 비우기\n",
    "        text_output.config(state=\"normal\")\n",
    "        text_output.delete(\"1.0\", END)\n",
    "        text_output.insert(END, answer)\n",
    "        text_output.config(state=\"disabled\")   # 출력창 편집 불가\n",
    "    else:\n",
    "        label_status.config(text=\"질문을 입력해주세요.\", foreground=\"red\")\n",
    "\n",
    "root = Tk()\n",
    "root.title('RAG 챗봇')\n",
    "root.geometry('500x700')\n",
    "root.resizable(False, False)\n",
    "# 전체 배경 연보라\n",
    "root.configure(bg=\"lavender\")\n",
    "# ====== 상단 이미지 영역 ======\n",
    "img = PhotoImage(file=\"source/hateslop.png\")\n",
    "label_img = Label(root, image=img, bg=\"white\")\n",
    "label_img.pack(pady=10)\n",
    "\n",
    "# ====== 입력 영역 ======\n",
    "frame_input = Frame(root, padx=10, pady=10)\n",
    "frame_input.pack(fill=\"x\")\n",
    "\n",
    "label_input = ttk.Label(frame_input, text=\"질문 입력\", font=(\"맑은 고딕\", 12, \"bold\"))\n",
    "label_input.pack(anchor=\"w\")\n",
    "\n",
    "text_input = Text(frame_input, height=6, font=(\"맑은 고딕\", 11))\n",
    "text_input.pack(pady=5)\n",
    "\n",
    "btn = ttk.Button(frame_input, text=\"전송\", command=process_query)\n",
    "btn.pack(pady=5)\n",
    "\n",
    "label_status = ttk.Label(frame_input, text=\"\", font=(\"맑은 고딕\", 10))\n",
    "label_status.pack(anchor=\"w\", pady=5)\n",
    "\n",
    "separator = ttk.Separator(root, orient=\"horizontal\")\n",
    "separator.pack(fill=\"x\", padx=10, pady=10)\n",
    "# ====== 출력 영역 ======\n",
    "frame_output = Frame(root, padx=10, pady=10)\n",
    "frame_output.pack(fill=\"both\", expand=True)\n",
    "\n",
    "label_output = ttk.Label(frame_output, text=\"답변\", font=(\"맑은 고딕\", 12, \"bold\"))\n",
    "label_output.pack(anchor=\"w\")\n",
    "\n",
    "text_output = Text(frame_output, wrap=\"word\", font=(\"맑은 고딕\", 11), state=\"disabled\", height=20, bg=\"#f9f9f9\")\n",
    "text_output.pack(side=\"left\", fill=\"both\", expand=True)\n",
    "\n",
    "scrollbar = ttk.Scrollbar(frame_output, command=text_output.yview)\n",
    "scrollbar.pack(side=\"right\", fill=\"y\")\n",
    "text_output.config(yscrollcommand=scrollbar.set)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
